INFO:__main__:Processed response from llama11b API : The image represents two encoder models: **encoder (NX)** and **encoder (NX)**. 

**Encoder NX**

*   This model is the basic encoder. It is located on the left side of the picture.
*   It consists of three main elements:
    1.  **Add & Norm**: This element represents the mean and variance calculation for each input feature. The output is the normalized representation.
    2.  **Feed Forward**: This element represents the neural network that feeds forward. The output is the dense representation.
    3.  **Multi-Head Attention**: This element represents the attention mechanism. It is used to attend to different parts of the input sequence. The output is the attention-weighted representation.

**Encoder NX**

*   The second encoder model is also located on the right side of the picture.
*   It is somewhat different from the basic encoder. It consists of seven main elements:
    1.  **Add & Norm**: This element is the same as the basic encoder.
    2.  **Feed Forward**: This element is also the same as the basic encoder.
    3.  **Multi-Head Attention**: This element is also the same as the basic encoder.
    4.  **Masked Multi-Head Attention**: This element represents the masked attention mechanism. It is used when the input sequence is partially visible.
    5.  **Add & Norm**: This element is the same as the basic encoder.
    6.  **Feed Forward**: This element is also the same as the basic encoder.
    7.  **Multi-Head Attention**: This element represents the multi-head attention mechanism. It is used to generate a set of different vectors.

Both encoders are used to process the input sequence and produce a set of vectors that can be used for further processing.
INFO:__main__:Processed response from llama90b API : There are two encoders marked in this diagram, one marked as "Encoder" and the other "NX Encoder." Each of these encoders consists of two layers, each labeled as "Add and Norm," "Feed Forward," and two subsequent "Add and Norm, Multi Head Attention." These layers are different between the two encoders; the "Encoder" feed forward and multi head attention function on the input "Add an Norm" while the "NX Encoder" feed forward and multi head attention functions on the previous "Add and Norm." The outputs can then either be outputted through positional encoding or the encoder outputs are used as inputs for the Decoder portion of the diagram. There is one, large rectangle outlined in orange for "the Decoder" with four layers that mirror NX Encoder, although it processes both the previous "Add and Norms" and the Encoder's outputs. These two blocks each get inputs straight from the input embeddeddings and outputs straight to the output embeddeddings.
{'llama11b': 'The image represents two encoder models: **encoder (NX)** and **encoder (NX)**. \n\n**Encoder NX**\n\n*   This model is the basic encoder. It is located on the left side of the picture.\n*   It consists of three main elements:\n    1.  **Add & Norm**: This element represents the mean and variance calculation for each input feature. The output is the normalized representation.\n    2.  **Feed Forward**: This element represents the neural network that feeds forward. The output is the dense representation.\n    3.  **Multi-Head Attention**: This element represents the attention mechanism. It is used to attend to different parts of the input sequence. The output is the attention-weighted representation.\n\n**Encoder NX**\n\n*   The second encoder model is also located on the right side of the picture.\n*   It is somewhat different from the basic encoder. It consists of seven main elements:\n    1.  **Add & Norm**: This element is the same as the basic encoder. \n    2.  **Feed Forward**: This element is also the same as the basic encoder. \n    3.  **Multi-Head Attention**: This element is also the same as the basic encoder. \n    4.  **Masked Multi-Head Attention**: This element represents the masked attention mechanism. It is used when the input sequence is partially visible. \n    5.  **Add & Norm**: This element is the same as the basic encoder. \n    6.  **Feed Forward**: This element is also the same as the basic encoder. \n    7.  **Multi-Head Attention**: This element represents the multi-head attention mechanism. It is used to generate a set of different vectors. \n\nBoth encoders are used to process the input sequence and produce a set of vectors that can be used for further processing.', 'llama90b': 'There are two encoders marked in this diagram, one marked as "Encoder" and the other "NX Encoder." Each of these encoders consists of two layers, each labeled as "Add and Norm," "Feed Forward," and two subsequent "Add and Norm, Multi Head Attention." These layers are different between the two encoders; the "Encoder" feed forward and multi head attention function on the input "Add an Norm" while the "NX Encoder" feed forward and multi head attention functions on the previous "Add and Norm." The outputs can then either be outputted through positional encoding or the encoder outputs are used as inputs for the Decoder portion of the diagram. There is one, large rectangle outlined in orange for "the Decoder" with four layers that mirror NX Encoder, although it processes both the previous "Add and Norms" and the Encoder\'s outputs. These two blocks each get inputs straight from the input embeddeddings and outputs straight to the output embeddeddings.'}